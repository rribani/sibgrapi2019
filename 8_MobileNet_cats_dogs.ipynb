{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rq0cLJjagt6u"
   },
   "source": [
    "# Experiment 8 - Demo example of negative transfer using a MobileNet pre-trained on Dogs vs Cats\n",
    "\n",
    "In this experiment we train a MobileNet model from scratch to perform classification using the Dogs vs Cats competition dataset from Kaggle (https://www.kaggle.com/c/dogs-vs-cats/data). \n",
    "\n",
    "This model is then used to transfer learning to task using a different domain of images, using the Chest X-Ray Dataset (https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/download).\n",
    "\n",
    "The models trained within the ImageNet dataset sometimes perform very good even with images in a very different domain, due the large number of features present in the ImageNet dataset. In this experiment we show a more evident case of negative transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2bKf6hEe0Jsn",
    "outputId": "aca4bb81-3bd4-412e-9078-abc205db1514"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "from os import walk\n",
    "from tqdm import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications import mobilenet\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Flatten, Dense, GlobalAveragePooling2D, Reshape, Conv2D, Dropout, Activation, MaxPooling2D\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpclNqU2gF6C"
   },
   "source": [
    "#### Set some variables to allow reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pw8rn-8NgFQL"
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(42)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "rn.seed(12345)\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "587xdFE8fvo1"
   },
   "source": [
    "## Download the Dogs vs Cats competition dataset from kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6sdcCQTgYg4"
   },
   "source": [
    "### Set kaggle username and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dpy9eyg3gWqy"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KAGGLE_USERNAME\"] = \"{username}\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"{key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrjm6-dmhHia"
   },
   "source": [
    "### Download using the Kaggle API\n",
    "\n",
    "https://www.kaggle.com/docs/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "J8_LgFJBqr0T",
    "outputId": "7066f207-bd2d-4587-a041-9aedf28216a8"
   },
   "outputs": [],
   "source": [
    "!mkdir dogs_vs_cats\n",
    "!kaggle competitions download -c dogs-vs-cats -p dogs_vs_cats\n",
    "!rm dogs_vs_cats/sampleSubmission.csv && rm dogs_vs_cats/test1.zip\n",
    "!unzip dogs_vs_cats/train.zip -d dogs_vs_cats\n",
    "!rm dogs_vs_cats/train.zip\n",
    "!mkdir dogs_vs_cats/dogs && mkdir dogs_vs_cats/cats\n",
    "!mv dogs_vs_cats/train/dog.* dogs_vs_cats/dogs && mv dogs_vs_cats/train/cat.* dogs_vs_cats/cats && rm -rf dogs_vs_cats/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HG73FcgThSon"
   },
   "source": [
    "## Organize data and split into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "4LP0V_FUi6qO",
    "outputId": "fd7a8786-7f6a-4a50-aa05-f96511eb0a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cats' 0 list([])]\n",
      " ['dogs' 1 list([])]]\n",
      "-----------------------\n",
      "Total images per class...\n",
      "cats : 12500\n",
      "dogs : 12500\n",
      "-----------------------\n",
      "Total of images: 25000\n",
      "-----------------------\n",
      "Total per set...\n",
      "x_train: 15000\n",
      "x_test: 5000\n",
      "x_val: 5000\n",
      "-----------------------\n",
      "File test.txt created.\n",
      "File val.txt created.\n",
      "File train.txt created.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'dogs_vs_cats'\n",
    "for (dirpath, dirnames, filenames) in walk(dataset_path):\n",
    "    if (len(dirnames) > 0):\n",
    "        folders_with_labels =  np.empty([len(dirnames), 3], dtype=object)\n",
    "        i = 0\n",
    "        for dir_name in sorted(dirnames, key=str.lower):\n",
    "            folders_with_labels[i][0] = dir_name\n",
    "            folders_with_labels[i][1] = i\n",
    "            folders_with_labels[i][2] = []\n",
    "            i += 1\n",
    "print(folders_with_labels)\n",
    "\n",
    "for path, label, imgs in folders_with_labels:\n",
    "    for (dirpath, dirnames, filenames) in walk(os.path.join(dataset_path, path)):\n",
    "        for file_name in filenames:\n",
    "            imgs.append('{}/{},{}'.format(path, file_name, label))\n",
    "            \n",
    "print('-----------------------')\n",
    "print(\"Total images per class...\")\n",
    "total = 0\n",
    "for path, label, imgs in folders_with_labels:\n",
    "    print(path, \":\", len(imgs))\n",
    "    total += len(imgs)\n",
    "print('-----------------------')\n",
    "print('Total of images:', total)\n",
    "\n",
    "split_validation = 2500\n",
    "split_test = 2500\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "for path, label, imgs in folders_with_labels:\n",
    "    test.extend(imgs[0:split_test])\n",
    "    val.extend(imgs[split_test:split_test+split_validation])\n",
    "    train.extend(imgs[split_test+split_validation:len(imgs)])\n",
    "    \n",
    "print('-----------------------')\n",
    "print('Total per set...')\n",
    "print('x_train:', len(train))\n",
    "print('x_test:', len(test))\n",
    "print('x_val:', len(val))\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(test)\n",
    "shuffle(val)\n",
    "shuffle(train)\n",
    "\n",
    "print('-----------------------')\n",
    "with open(os.path.join(dataset_path, 'test.txt'), 'w') as f:  \n",
    "    f.writelines(\"%s\\n\" % item for item in test)\n",
    "    print('File test.txt created.')\n",
    "    \n",
    "with open(os.path.join(dataset_path, 'val.txt'), 'w') as f:  \n",
    "    f.writelines(\"%s\\n\" % item for item in val)\n",
    "    print('File val.txt created.')\n",
    "    \n",
    "with open(os.path.join(dataset_path, 'train.txt'), 'w') as f:  \n",
    "    f.writelines(\"%s\\n\" % item for item in train)\n",
    "    print('File train.txt created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYVf3YCd0rUn"
   },
   "source": [
    "## Set initial parameters, functions and generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXbAIEHA0ud4"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'dogs_vs_cats'\n",
    "width = 224\n",
    "height = 224\n",
    "input_shape = (width, height, 3)\n",
    "batch_size = 10\n",
    "verbose = 1\n",
    "classes_dict = {\n",
    "    0: 'cat',\n",
    "    1: 'dog'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xfOnWLj0y3_"
   },
   "source": [
    "Here we define 3 functions:\n",
    "\n",
    "\n",
    "*   read_training_file() : Read the paths to images and labels for the dataset using each file (train, validation and test).\n",
    "*   preprocess_image() : Preprocess the images in batch, set the size, the type and put in the TF format (scale pixels between -1 and 1).\n",
    "*   data_generator() : Returns a generator that reads the images in batch and call the preprocess function, to be used in the training process with the fit_generator function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynNV2V4t0zyy"
   },
   "outputs": [],
   "source": [
    "def read_training_file(path, separator=' '):\n",
    "    images = []\n",
    "    labels = []\n",
    "    with open(path, 'r') as f:\n",
    "        for i in tqdm(f.readlines()):\n",
    "            img_name, cls = i.strip().split(separator)\n",
    "            cls = int(cls)\n",
    "            images.append(img_name)\n",
    "            labels.append(cls)\n",
    "    return images, labels\n",
    "\n",
    "def preprocess_image(images, images_path, width, height):\n",
    "    pp_images = []\n",
    "\n",
    "    for image in images:\n",
    "        img = cv2.imread(os.path.join(images_path, image))\n",
    "        if img.shape[0] != 224 or img.shape[1] != 224:\n",
    "            img = cv2.resize(img, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "        img = np.asarray(img, 'float32')\n",
    "        img = preprocess_input(img, mode='tf')\n",
    "        pp_images.append(img)\n",
    "\n",
    "    return np.asarray(pp_images)\n",
    "\n",
    "def data_generator(images, labels, batch_size, images_path, width, height):\n",
    "    batch_start = 0\n",
    "    while batch_start < len(images):\n",
    "        batch_end = min(batch_start + batch_size, len(images))\n",
    "        y_batch = labels[batch_start : batch_end]\n",
    "        x_batch = preprocess_image(images[batch_start : batch_end],\n",
    "                                   images_path,\n",
    "                                   width,\n",
    "                                   height)\n",
    "        batch_start += batch_size\n",
    "        if batch_start >= len(images):\n",
    "            batch_start = 0\n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1St2NWaV03k-"
   },
   "source": [
    "Read the label files (train, val and test) and set two arrays:\n",
    "\n",
    "1.   paths to images.\n",
    "2.   labels as a binary class matrix, for use with categorical_crossentropy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "_30G4LRc04D3",
    "outputId": "b009b40e-3945-43d7-ada5-f42a022e22c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:00<00:00, 845375.83it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 628982.06it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 842703.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 15000 documents\n",
      "Validation: 5000 documents\n",
      "Test: 5000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(dataset_path, 'train.txt')\n",
    "val_path = os.path.join(dataset_path, 'val.txt')\n",
    "test_path = os.path.join(dataset_path, 'test.txt')\n",
    "\n",
    "train_images, train_labels = read_training_file(train_path, ',')\n",
    "val_images, val_labels = read_training_file(val_path, ',')\n",
    "test_images, test_labels = read_training_file(test_path, ',')\n",
    "\n",
    "print('Train: {} documents'.format(len(train_images)))\n",
    "print('Validation: {} documents'.format(len(val_images)))\n",
    "print('Test: {} documents'.format(len(test_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBpXTJfB06Un"
   },
   "source": [
    "Create generators for the training set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2I0-rJSe07-V"
   },
   "outputs": [],
   "source": [
    "train_gen = data_generator(train_images, train_labels, batch_size, dataset_path, width, height)\n",
    "val_gen = data_generator(val_images, val_labels, batch_size, dataset_path, width, height)\n",
    "\n",
    "num_batches_per_epoch = int(round(len(train_images) / float(batch_size)))\n",
    "num_batches_per_epoch_val = int(round(len(val_images) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lhq1rYayu8Yx"
   },
   "source": [
    "## 1 - Training MobileNet with regularization + augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQxRL38XnIcx"
   },
   "source": [
    "### Create augmentation generator\n",
    "\n",
    "Here we use the ImageDataGenerator from Keras to apply different types of augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79hgSqqyvDuD"
   },
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(zoom_range=0.3, \n",
    "                               rotation_range=50,\n",
    "                               width_shift_range=0.2, \n",
    "                               height_shift_range=0.2, \n",
    "                               shear_range=0.2, \n",
    "                               horizontal_flip=True, \n",
    "                               fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdaMzbdMvRHn"
   },
   "outputs": [],
   "source": [
    "def augmentation_generator(in_generator):\n",
    "\n",
    "    for in_x, in_y in in_generator:\n",
    "        g_x = image_gen.flow(in_x, in_y,\n",
    "                             batch_size=in_x.shape[0])\n",
    "        x, y = next(g_x)\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_pBTtTFvsJb"
   },
   "outputs": [],
   "source": [
    "train_gen = augmentation_generator(train_gen)\n",
    "val_gen = augmentation_generator(val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxL162EJq_Q2"
   },
   "source": [
    "### Define and compile the MobileNet model architecture\n",
    "\n",
    "Here we define and compile the MobileNet-based model architecture using alpha=0.5 with a GlobalAveragePooling + Conv2D 1x1 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "id": "8CVAYg4Zq8GG",
    "outputId": "541ca7a4-14d7-45ee-b6b7-bef9da8154bb"
   },
   "outputs": [],
   "source": [
    "base_mobilenet = mobilenet.MobileNet(include_top=False, alpha=0.5, weights=None, input_shape=input_shape)\n",
    "\n",
    "x = base_mobilenet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Reshape((1, 1, 512), name='reshape_1')(x)\n",
    "x = Dropout(0.3, name='dropout')(x)\n",
    "x = Conv2D(1, (1, 1),\n",
    "           padding='same', name='conv_preds')(x)\n",
    "x = Activation('sigmoid', name='act_sigmoid')(x)\n",
    "x = Reshape((1,), name='reshape_2')(x)\n",
    "\n",
    "model = Model(inputs=base_mobilenet.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "_zyVVoT07kHD",
    "outputId": "d0a71f41-c5fa-4ac3-8209-d41f1dee00ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('8_mobilenet_dogs_cats.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hw-ASHE57vfl"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.SGD(lr=0.02), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yl2XqK8lnCMM"
   },
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqDqi2Fe32yW"
   },
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "\n",
    "print('Start training...')\n",
    "history = model.fit_generator(train_gen,\n",
    "                                  steps_per_epoch=num_batches_per_epoch,\n",
    "                                  epochs=num_epochs,\n",
    "                                  verbose=verbose,\n",
    "                                  validation_data=val_gen,\n",
    "                                  validation_steps=num_batches_per_epoch_val)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V7A1VbAl_8FY"
   },
   "source": [
    "Train acc: 92.29%\n",
    "\n",
    "Val acc: 91.97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xZdPfGdzZZa8",
    "outputId": "6f5d8f3d-5b2d-4d5f-e2a1-44e243247902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "model.save('mobilenet_dogs_cats_60epochs.h5')\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnsp2GkZm91E"
   },
   "source": [
    "### Plot training metrics \n",
    "\n",
    "Plot accuracy and loss for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnDyr9x374a5"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Accuracy')\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylim((0.0, 3.0))\n",
    "plt.legend(['Train', 'Val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22y-g5SQTnco"
   },
   "source": [
    "### Evaluate the trained model with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "RFHUYoLjTonI",
    "outputId": "40ea2534-97ad-4587-cead-7a1930af45bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating trained model...\n",
      "Finished mobilenet.evaluate_generator\n",
      "['loss', 'acc']\n",
      "[0.20123412051331252, 0.9197999929189682]\n"
     ]
    }
   ],
   "source": [
    "num_batches_per_epoch_test = int(round(len(test_images) / float(batch_size)))\n",
    "\n",
    "print('Evaluating trained model...')\n",
    "result = model.evaluate_generator(generator=data_generator(test_images,\n",
    "                                                               test_labels,\n",
    "                                                               batch_size,\n",
    "                                                               dataset_path, width, height),\n",
    "                                      steps=num_batches_per_epoch_test)\n",
    "\n",
    "print(\"Finished mobilenet.evaluate_generator\")\n",
    "print(model.metrics_names)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztzSmXbP2JWx"
   },
   "source": [
    "## 2 - Training a MobileNet from scratch to classify the Chest X-Ray Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzaL14cfuV4e"
   },
   "source": [
    "### Download the Dataset using the Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_c-soEbHaK8Q",
    "outputId": "5e94933b-14f0-4b6e-a323-056f7ce0c6f2"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
    "!unzip chest-xray-pneumonia.zip\n",
    "!rm chest-xray-pneumonia.zip\n",
    "!rm -rf chest_xray/__MACOSX/\n",
    "!rm -rf chest_xray/chest_xray/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JcUy3E3Su-9w"
   },
   "source": [
    "### Organize data and split into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "8mSUeP7Ua9yu",
    "outputId": "64eb8721-f114-46f5-ac02-28fde7c57f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Set: train\n",
      "chest_xray/train\n",
      "[['NORMAL' 0 list([])]\n",
      " ['PNEUMONIA' 1 list([])]]\n",
      "\n",
      "Total images per class...\n",
      "NORMAL : 1341\n",
      "PNEUMONIA : 3875\n",
      "\n",
      "Total of images: 5216\n",
      "images: 5216\n",
      "-----------------------\n",
      "Set: val\n",
      "chest_xray/val\n",
      "[['NORMAL' 0 list([])]\n",
      " ['PNEUMONIA' 1 list([])]]\n",
      "\n",
      "Total images per class...\n",
      "NORMAL : 8\n",
      "PNEUMONIA : 8\n",
      "\n",
      "Total of images: 16\n",
      "images: 16\n",
      "-----------------------\n",
      "Set: test\n",
      "chest_xray/test\n",
      "[['NORMAL' 0 list([])]\n",
      " ['PNEUMONIA' 1 list([])]]\n",
      "\n",
      "Total images per class...\n",
      "NORMAL : 234\n",
      "PNEUMONIA : 390\n",
      "\n",
      "Total of images: 624\n",
      "images: 624\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'chest_xray'\n",
    "for dset in ['train','val','test']:\n",
    "    print('-----------------------')\n",
    "    print('Set:', dset)\n",
    "    set_path = os.path.join(dataset_path, dset)\n",
    "    print(set_path)\n",
    "\n",
    "    for (dirpath, dirnames, filenames) in walk(set_path):\n",
    "        if (len(dirnames) > 0):\n",
    "            folders_with_labels = np.empty([len(dirnames), 3], dtype=object)\n",
    "            i = 0\n",
    "            for dir_name in sorted(dirnames, key=str.lower):\n",
    "                folders_with_labels[i][0] = dir_name\n",
    "                folders_with_labels[i][1] = i\n",
    "                folders_with_labels[i][2] = []\n",
    "                i += 1\n",
    "    print(folders_with_labels)\n",
    "\n",
    "    # Fill image paths in the array from images in the folders\n",
    "    for path, label, imgs in folders_with_labels:\n",
    "        for (dirpath, dirnames, filenames) in walk(os.path.join(set_path, path)):\n",
    "            for file_name in filenames:\n",
    "                imgs.append('{}/{}/{},{}'.format(dset, path, file_name, label))\n",
    "\n",
    "    print('')\n",
    "    print(\"Total images per class...\")\n",
    "    total = 0\n",
    "    for path, label, imgs in folders_with_labels:\n",
    "        print(path, \":\", len(imgs))\n",
    "        total += len(imgs)\n",
    "    print('')\n",
    "    print('Total of images:', total)\n",
    "\n",
    "    all_imgs = []\n",
    "    for path, label, imgs in folders_with_labels:\n",
    "        all_imgs.extend(imgs)\n",
    "    print('images:', len(all_imgs))\n",
    "\n",
    "    shuffle(all_imgs)\n",
    "\n",
    "    with open(os.path.join(dataset_path, '{}.txt'.format(dset)), 'w') as f:  \n",
    "        f.writelines(\"%s\\n\" % item for item in all_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCX-VX-T1PYD"
   },
   "source": [
    "### Set initial parameters and generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gn0jlwDRbVRa"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'chest_xray'\n",
    "width = 224\n",
    "height = 224\n",
    "input_shape = (width, height, 3)\n",
    "batch_size = 10\n",
    "verbose = 1\n",
    "classes_dict = {\n",
    "    0: 'NORMAL',\n",
    "    1: 'PNEUMONIA'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "RwTfAOkFbXq_",
    "outputId": "790c084e-12df-42ab-c5a0-5b438f9bbb28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5216/5216 [00:00<00:00, 555830.53it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 5398.07it/s]\n",
      "100%|██████████| 624/624 [00:00<00:00, 251730.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5216 documents\n",
      "Validation: 16 documents\n",
      "Test: 624 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(dataset_path, 'train.txt')\n",
    "val_path = os.path.join(dataset_path, 'val.txt')\n",
    "test_path = os.path.join(dataset_path, 'test.txt')\n",
    "\n",
    "train_images, train_labels = read_training_file(train_path, ',')\n",
    "val_images, val_labels = read_training_file(val_path, ',')\n",
    "test_images, test_labels = read_training_file(test_path, ',')\n",
    "\n",
    "print('Train: {} documents'.format(len(train_images)))\n",
    "print('Validation: {} documents'.format(len(val_images)))\n",
    "print('Test: {} documents'.format(len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vo8qAll7bZc4"
   },
   "outputs": [],
   "source": [
    "train_gen = data_generator(train_images, train_labels, batch_size, dataset_path, width, height)\n",
    "val_gen = data_generator(val_images, val_labels, batch_size, dataset_path, width, height)\n",
    "\n",
    "num_batches_per_epoch = int(round(len(train_images) / float(batch_size)))\n",
    "num_batches_per_epoch_val = int(round(len(val_images) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qbvUOn-2paH"
   },
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kM9fTS7bdoSX"
   },
   "outputs": [],
   "source": [
    "base_mobilenet = mobilenet.MobileNet(include_top=False, alpha=0.5, weights=None, input_shape=input_shape)\n",
    "\n",
    "x = base_mobilenet.output\n",
    "x = GlobalAveragePooling2D(name='gap_xray')(x)\n",
    "x = Reshape((1, 1, 512), name='reshape_1_xray')(x)\n",
    "x = Dropout(0.3, name='dropout_xray')(x)\n",
    "x = Conv2D(1, (1, 1), padding='same', name='conv_preds_xray')(x)\n",
    "x = Activation('sigmoid', name='act_sigmoid_xray')(x)\n",
    "x = Reshape((1,), name='reshape_2_xray')(x)\n",
    "\n",
    "model = Model(inputs=base_mobilenet.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tspithcrdvvN"
   },
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('8_mobilenet_xray_from_scratch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDuLcj88d2rm"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.SGD(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xa-tNNAN2sfu"
   },
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "NSkJ3JTmdqJE",
    "outputId": "8d210b3a-bfd2-41b0-f4b7-fe278cacf088"
   },
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "print('Start training...')\n",
    "history = model.fit_generator(train_gen,\n",
    "                                  steps_per_epoch=num_batches_per_epoch,\n",
    "                                  epochs=num_epochs,\n",
    "                                  verbose=verbose,\n",
    "                                  validation_data=val_gen,\n",
    "                                  validation_steps=num_batches_per_epoch_val)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train: 99.98%\n",
    "\n",
    "Val: 81.25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xTKtJ9CteTCI",
    "outputId": "30e813c5-ae42-4d19-b24d-7a672955dbc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# model.save('mobilenet_xray_from_scratch.h5')\n",
    "# print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZuM-Cii2uEx"
   },
   "source": [
    "### Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "bxOA_r2xds1K",
    "outputId": "2cb32a8c-8ce1-48a6-a93a-b9b461a62504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating trained model...\n",
      "Finished mobilenet.evaluate_generator\n",
      "['loss', 'acc']\n",
      "[0.9401362669549042, 0.7693548370753566]\n"
     ]
    }
   ],
   "source": [
    "num_batches_per_epoch_test = int(round(len(test_images) / float(batch_size)))\n",
    "\n",
    "print('Evaluating trained model...')\n",
    "result = model.evaluate_generator(generator=data_generator(test_images,\n",
    "                                                               test_labels,\n",
    "                                                               batch_size,\n",
    "                                                               dataset_path, width, height),\n",
    "                                      steps=num_batches_per_epoch_test)\n",
    "\n",
    "print(\"Finished mobilenet.evaluate_generator\")\n",
    "print(model.metrics_names)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BI0oagpLugA0"
   },
   "source": [
    "## 3 - Training a MobileNet to classify the Chest X-Ray Dataset using the feature extraction from Dogs vs Cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrUumnBG1UGn"
   },
   "source": [
    "### Load the MobileNet model trained on Dogs vs Cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7hG5o2vaXzm"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "base_dogs_cats = load_model('8_mobilenet_dogs_cats.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPnePkUZ9gcX"
   },
   "outputs": [],
   "source": [
    "base_dogs_cats.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-od2Zps1b-w"
   },
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mkGd3l5abQD"
   },
   "outputs": [],
   "source": [
    "x = base_dogs_cats.get_layer('conv_pw_13_relu').output\n",
    "\n",
    "x = GlobalAveragePooling2D(name='gap_xray')(x)\n",
    "x = Reshape((1, 1, 512), name='reshape_1_xray')(x)\n",
    "x = Dropout(0.3, name='dropout_xray')(x)\n",
    "x = Conv2D(1, (1, 1), padding='same', name='conv_preds_xray')(x)\n",
    "x = Activation('sigmoid', name='act_sigmoid_xray')(x)\n",
    "x = Reshape((1,), name='reshape_2_xray')(x)\n",
    "\n",
    "model = Model(inputs=base_dogs_cats.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIHVU6EfsGZl"
   },
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('8_mobilenet_xray_from_dogs_cats.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IK0QZ0cZ1wLp"
   },
   "source": [
    "### Set trainable and non-trainable layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MUZPeB1QN_2k",
    "outputId": "da4bd15a-8edb-462b-f5a5-a22bfa199276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not trainable\n",
      "<keras.engine.input_layer.InputLayer object at 0x7fec3af855c0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fec3af85630>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3af85828>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3af85668>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3af85978>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3af85a58>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3af85a90>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3af85b00>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3af85dd8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3af85e10>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3af85f98>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fec3cebc128>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3cebc160>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cebc1d0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cebc4a8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3cebc4e0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cebc668>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cebc780>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3cebc7b8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cebc828>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cebcb00>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3cebcb38>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cebccc0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cebcdd8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fec3cebce10>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3cebce48>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cebceb8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3af85fd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3cec3208>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cec3390>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cec34a8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3cec34e0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cec3550>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cec3828>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3cec3860>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cec39e8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cec3b00>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fec3cec3b38>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3cec3b70>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cec3be0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cec3eb8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3cec3ef0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ceca0b8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ceca1d0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3ceca208>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ceca278>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ceca550>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3ceca588>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ceca710>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ceca828>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3ceca860>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ceca8d0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cecaba8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3cecabe0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3cecad68>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3cecae80>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3cecaeb8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce4f048>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce4f240>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3ce4f278>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce4f400>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce4f518>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3ce4f550>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce4f5c0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce4f898>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3ce4f8d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce4fa58>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce4fb70>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3ce4fba8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce4fc18>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce4fef0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3ce4ff28>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce550f0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce55208>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fec3ce55240>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3ce55278>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce552e8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce555c0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3ce555f8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce55780>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce55898>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fec3ce558d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce55940>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce55c18>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3ce55c50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fec3ce55dd8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fec3ce55ef0>\n",
      "\n",
      "Trainable\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7fec3ce55f28> gap_xray\n",
      "<keras.layers.core.Reshape object at 0x7fec3ce55f98> reshape_1_xray\n",
      "<keras.layers.core.Dropout object at 0x7fec3cebcfd0> dropout_xray\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fec3ce5d048> conv_preds_xray\n",
      "<keras.layers.core.Activation object at 0x7fec3ce5d1d0> act_sigmoid_xray\n",
      "<keras.layers.core.Reshape object at 0x7fec3ce5d208> reshape_2_xray\n"
     ]
    }
   ],
   "source": [
    "print('Not trainable')\n",
    "for layer in model.layers[:87]:\n",
    "    print(layer)\n",
    "    layer.trainable = False\n",
    "print('')\n",
    "print('Trainable')\n",
    "for layer in model.layers[87:]:\n",
    "    print(layer, layer.name)\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q625xSWBOBog"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.SGD(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4azsJLTk2Dcu"
   },
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "teIzwPCadzEC",
    "outputId": "04382918-6675-4268-b0a3-52a56b3d58f4"
   },
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "print('Start training...')\n",
    "history = model.fit_generator(train_gen,\n",
    "                                  steps_per_epoch=num_batches_per_epoch,\n",
    "                                  epochs=num_epochs,\n",
    "                                  verbose=verbose,\n",
    "                                  validation_data=val_gen,\n",
    "                                  validation_steps=num_batches_per_epoch_val)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train: 80.38%\n",
    "\n",
    "Val: 50.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nc5LTo4x1KIi",
    "outputId": "8503e89e-34f7-4bbf-a276-3b5d965e9ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# model.save('mobilenet_xray_from_dogs_cats.h5')\n",
    "# print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6FrWU9d2wbH"
   },
   "source": [
    "### Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "75n02FDhd2ka",
    "outputId": "ad4f811a-293b-44ee-d35c-12402751048a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating trained model...\n",
      "Finished mobilenet.evaluate_generator\n",
      "['loss', 'acc']\n",
      "[0.9360114345627446, 0.62580645432876]\n"
     ]
    }
   ],
   "source": [
    "num_batches_per_epoch_test = int(round(len(test_images) / float(batch_size)))\n",
    "\n",
    "print('Evaluating trained model...')\n",
    "result = model.evaluate_generator(generator=data_generator(test_images,\n",
    "                                                               test_labels,\n",
    "                                                               batch_size,\n",
    "                                                               dataset_path, width, height),\n",
    "                                      steps=num_batches_per_epoch_test)\n",
    "\n",
    "print(\"Finished mobilenet.evaluate_generator\")\n",
    "print(model.metrics_names)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "8_MobileNet_cats_dogs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
